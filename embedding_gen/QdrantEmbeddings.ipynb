{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cbfee7",
   "metadata": {},
   "source": [
    "Currently no picutures are readin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d795c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# def extract_pdf_pages(pdf_path):\n",
    "#     reader = PdfReader(pdf_path)\n",
    "#     pages = []\n",
    "#     for i, page in enumerate(reader.pages):\n",
    "#         text = page.extract_text()\n",
    "#         if not text:\n",
    "#             text = \"Empty\"\n",
    "#         pages.append({'page_number': i+1, 'text': text})\n",
    "#     return pages\n",
    "\n",
    "# pdf_pages = extract_pdf_pages(\"Makeathon TUM presentation.pdf\")\n",
    "\n",
    "# pdf_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281fcec",
   "metadata": {},
   "source": [
    "### OpenAI Vision API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "275389e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Text: Empty...\n",
      "============================================================\n",
      "Page 2:\n",
      "Text: Flow\n",
      "Speaks2) Text Analyzer \n",
      "for keywords1) Speech to \n",
      "text converter3) Slide mover\n",
      "Presentation \n",
      "sl...\n",
      "============================================================\n",
      "Page 3:\n",
      "Text: Flow\n",
      "Speaks2) Text Analyzer \n",
      "for keywords1) Speech to \n",
      "text converter3a) Slide \n",
      "mover\n",
      "Presentation \n",
      "...\n",
      "============================================================\n",
      "Page 4:\n",
      "Text: Structure of presentation\n",
      "•Introduction (ideally something funny)\n",
      "•What problem is our product solvi...\n",
      "============================================================\n",
      "Page 5:\n",
      "Text: Introduction (ideally something funny)\n",
      "•30MM (Power Point) presentations are created per day\n",
      "•https:...\n",
      "============================================================\n",
      "Page 6:\n",
      "Text: What problem is our product solving?\n",
      "•Presentation have a fixed storyline and do not allow the prese...\n",
      "============================================================\n",
      "Page 7:\n",
      "Text: What solutions already exist?\n",
      "...\n",
      "Image 1 description: **Description:**\n",
      "\n",
      "This image is from the Intuiface platform and showcases **Voice-Controlled Present...\n",
      "Image 2 description: **Description:**\n",
      "\n",
      "The image shows the logo for \"presentations.AI.\" It features a rounded square icon...\n",
      "Image 3 description: **Brief Description:**\n",
      "\n",
      "The image introduces \"The World's Best AI Presentation Maker,\" highlighting ...\n",
      "============================================================\n",
      "Page 8:\n",
      "Text: Sources for solutions that already exist\n",
      "•https://intuiville.intuiface.com/usage/voice -controlled -...\n",
      "Image 1 description: The table compares four AI presentation tools based on features like real-time slide creation, voice...\n",
      "============================================================\n",
      "Page 9:\n",
      "Text: Why is our solution better?\n",
      "•Jumping between slides with voice commands (Let ´s switch back \n",
      "to the ...\n",
      "============================================================\n",
      "Page 10:\n",
      "Text: Team\n",
      "•Meet the team\n",
      "•Johannes, Sophia, Woojin, Ben...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from each page of a PDF.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            text = \"Empty\"\n",
    "        pages.append({'page_number': i + 1, 'text': text})\n",
    "    return pages\n",
    "\n",
    "def extract_images_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts images from each page of a PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i in range(len(doc)):\n",
    "        for img in doc[i].get_images(full=True):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append({\"page_number\": i + 1, \"image\": image})\n",
    "    return images\n",
    "\n",
    "def describe_image_with_gpt4v(image):\n",
    "    \"\"\"Sends a PIL Image to GPT-4.1 and returns a description.\"\"\"\n",
    "    # Convert the image to bytes\n",
    "    img_byte_array = io.BytesIO()\n",
    "    image.save(img_byte_array, format=\"PNG\")\n",
    "    img_bytes = img_byte_array.getvalue()\n",
    "\n",
    "    # Encode the image in base64\n",
    "    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "    # Prepare the image data in the required format\n",
    "    image_data = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send the image to GPT-4 Vision\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \n",
    "                     # prompt for the image\n",
    "                     \"text\": \"Describe briefly, mention diagrams or tables and also include any relevant details. Keep it concise. \"\n",
    "                     \"This is used in combination with text from the PDF.\"}, \n",
    "                    image_data\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    # Extract and return the description\n",
    "    description = response.choices[0].message.content\n",
    "    return description\n",
    "\n",
    "def combine_text_and_images(pdf_path):\n",
    "    \"\"\"Combines text extraction and image descriptions page-by-page.\"\"\"\n",
    "    text_pages = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "\n",
    "    # Organize images by page number\n",
    "    images_by_page = {}\n",
    "    for img_data in images:\n",
    "        page_num = img_data[\"page_number\"]\n",
    "        if page_num not in images_by_page:\n",
    "            images_by_page[page_num] = []\n",
    "        images_by_page[page_num].append(img_data[\"image\"])\n",
    "\n",
    "    # Combine text + image descriptions\n",
    "    combined_pages = []\n",
    "    for page in text_pages:\n",
    "        page_number = page[\"page_number\"]\n",
    "        text = page[\"text\"]\n",
    "\n",
    "        image_descriptions = []\n",
    "        if page_number in images_by_page:\n",
    "            for image in images_by_page[page_number]:\n",
    "                description = describe_image_with_gpt4v(image)\n",
    "                image_descriptions.append(description)\n",
    "\n",
    "        combined_pages.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"text\": text,\n",
    "            \"image_descriptions\": image_descriptions\n",
    "        })\n",
    "\n",
    "    return combined_pages\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Makeathon TUM presentation.pdf\"\n",
    "final_pages = combine_text_and_images(pdf_path)\n",
    "\n",
    "# Print the combined result\n",
    "for page in final_pages:\n",
    "    print(f\"Page {page['page_number']}:\")\n",
    "    print(f\"Text: {page['text'][:100]}...\")  # Print first 100 chars\n",
    "    if page[\"image_descriptions\"]:\n",
    "        for idx, desc in enumerate(page[\"image_descriptions\"]):\n",
    "            print(f\"Image {idx+1} description: {desc[:100]}...\")  # Print first 100 chars\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92d545",
   "metadata": {},
   "source": [
    "### Huggingface: https://huggingface.co/sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b046ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')  # You can pick another\n",
    "# dim = model.get_sentence_embedding_dimension()  # <<<<<<<< GET DIMENSION EARLY\n",
    "# print(f\"Embedding dimension: {dim}\")\n",
    "\n",
    "# def embed_batch(pdf_pages):\n",
    "#     for page in pdf_pages:\n",
    "#         if page['text']:\n",
    "#             page['embedding'] = model.encode(page['text'])\n",
    "#     return [page['embedding'] for page in pdf_pages if 'embedding' in page]  # type: ignore\n",
    "\n",
    "# # After extracting pdf_pages\n",
    "# embeddings = embed_batch(pdf_pages)\n",
    "# print(f\"Embedded {len(embeddings)} pages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb1387",
   "metadata": {},
   "source": [
    "### OPENAI Alternative: https://platform.openai.com/docs/guides/embeddings?lang=python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "816265ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 10 pages, each a 1536-dim vector\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()                        # make sure OPENAI_API_KEY is in your .env\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()                   # reads api key from env\n",
    "\n",
    "def embed_batch(pdf_pages, batch_size: int = 16):\n",
    "    # 1) gather all page texts (empty string if missing)\n",
    "    texts = [page.get('text', '') for page in pdf_pages]\n",
    "\n",
    "    # 2) call the API in chunks of batch_size\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i : i + batch_size]\n",
    "        resp = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=chunk\n",
    "        )\n",
    "        # resp.data is a list of objects, each with an .embedding list\n",
    "        all_embeddings.extend([d.embedding for d in resp.data])\n",
    "\n",
    "    # 3) assign back to pages\n",
    "    for page, emb in zip(pdf_pages, all_embeddings):\n",
    "        if page.get('text'):\n",
    "            page['embedding'] = emb\n",
    "        else:\n",
    "            page['embedding'] = None\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# — example usage —\n",
    "# after you’ve built pdf_pages = [ {\"page_number\":1, \"text\": \"...\"} , ... ]\n",
    "embs = embed_batch(pdf_pages)\n",
    "dim = len(embs[0])  # <<<<<<<< GET DIMENSION EARLY\n",
    "print(f\"Embedded {len(embs)} pages, each a {len(embs[0])}-dim vector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691e484",
   "metadata": {},
   "source": [
    "### DELETE collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6cacfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(collection_name=\"pdf_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "12043f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# client.delete_collection(collection_name=\"pdf_collection\")\n",
    "\n",
    "COLL = os.getenv(\"QDRANT_COLLECTION_NAME\", \"pdf_collection\")\n",
    "\n",
    "# 5) Create collection if it doesn’t exist\n",
    "collections = client.get_collections().collections\n",
    "exists = any(c.name == COLL for c in collections)\n",
    "\n",
    "if not exists:\n",
    "    client.create_collection(\n",
    "        collection_name=COLL,\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "237b7244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted 10 pages (dim=1536) into “pdf_collection2”.\n"
     ]
    }
   ],
   "source": [
    "# 6) Prepare your points and upsert\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=page[\"page_number\"],\n",
    "        vector=page[\"embedding\"],\n",
    "        payload={\n",
    "            \"text\":        page[\"text\"],\n",
    "            \"page_number\": page[\"page_number\"],\n",
    "        },\n",
    "    )\n",
    "    for page in pdf_pages\n",
    "]\n",
    "\n",
    "client.upsert(collection_name=COLL, points=points)\n",
    "print(f\"✅ Upserted {len(points)} pages (dim={dim}) into “{COLL}”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9715c67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Qdrant collection “pdf_collection2”.\n",
      "Page 7   score=0.603\n",
      " → 'What solutions already exist? '\n",
      "\n",
      "Page 8   score=0.403\n",
      " → 'Sources for solutions that already exist •https://intuiville.intuiface.com/usage/voice -controlled -presentations -work •https://www.presentations.ai/ '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# — clients & globals —\n",
    "openai_client   = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "qdrant          = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\", \"pdf_collection\")\n",
    "print(f\"Using Qdrant collection “{COLLECTION_NAME}”.\")\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 2):\n",
    "    # 1️⃣ Embed your query with OpenAI\n",
    "    resp = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=[query],\n",
    "    )\n",
    "    query_vector = resp.data[0].embedding\n",
    "\n",
    "    # 2️⃣ Fire the new Query API\n",
    "    qr = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_vector,   # your dense vector\n",
    "        limit=top_k,          # how many neighbors to return\n",
    "        with_payload=True,    # pull back your stored page text & page_number\n",
    "    )                       \n",
    "\n",
    "    # 3️⃣ Extract the list of ScoredPoint objects\n",
    "    scored_points = qr.points  # type: ignore[attr-defined] :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "    # 4️⃣ Format the results\n",
    "    results = []\n",
    "    for sp in scored_points:\n",
    "        results.append({\n",
    "            \"score\":        sp.score,\n",
    "            \"page_number\":  sp.payload.get(\"page_number\"),\n",
    "            \"text_snippet\": sp.payload.get(\"text\", \"\")[:200].replace(\"\\n\", \" \")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    for r in semantic_search(\"What solution?\", top_k=2):\n",
    "        print(f\"Page {r['page_number']}   score={r['score']:.3f}\")\n",
    "        print(f\" → {r['text_snippet']!r}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
