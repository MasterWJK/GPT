{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cbfee7",
   "metadata": {},
   "source": [
    "Currently no picutures are readin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d795c888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1, 'text': 'Empty'},\n",
       " {'page_number': 2,\n",
       "  'text': 'Flow\\nSpeaks2) Text Analyzer \\nfor keywords1) Speech to \\ntext converter3) Slide mover\\nPresentation \\nslide changes\\nPresentation \\nslide changes5) Slide \\nkeyword \\nclassifier4) Text \\nextractor4) Image to text \\nconverter\\n6) Presentation \\nslide number to \\nkeyword tableLive Presentation app\\nOffline presentation classifier'},\n",
       " {'page_number': 3,\n",
       "  'text': 'Flow\\nSpeaks2) Text Analyzer \\nfor keywords1) Speech to \\ntext converter3a) Slide \\nmover\\nPresentation \\nslide changesLive Presentation app with slide creator\\n3b) Slide creator\\n3.2) Google \\ninfo finder3.1) Slide \\ndesign3.3) Adds slide \\nto presentatio'},\n",
       " {'page_number': 4,\n",
       "  'text': 'Structure of presentation\\n•Introduction (ideally something funny)\\n•What problem is our product solving?\\n•What solutions already exist?\\n•Why is our solution better?\\n•How big is the business opportunity?\\n•Business Model\\n•Team'},\n",
       " {'page_number': 5,\n",
       "  'text': 'Introduction (ideally something funny)\\n•30MM (Power Point) presentations are created per day\\n•https://presentationpoint.com/an -estimation -of-daily -usage -globally -of-\\npowerpoint/#:~:text=Estimates%20have%20floated%20around%20the,presentations%20cr\\neated%20every%20single%20day .'},\n",
       " {'page_number': 6,\n",
       "  'text': 'What problem is our product solving?\\n•Presentation have a fixed storyline and do not allow the presenter \\nto adjust in -flight to needs of the audience.\\n•Example 1: Professor realizes studes don ´t have the required \\nbackground and needs to go into more detail on a specific section\\n•Example 2: Nvideo CEO presents at a large conference and in the \\nFAQ sessions, someone asks about an early topic and the \\npresenter has to click the back button on his presenter 100 times.\\n•Example 3: Sales team presents quarterly business review to the \\nCEO, who is interested to also see previous year numbers (which \\nthe team does not have a slide for)'},\n",
       " {'page_number': 7, 'text': 'What solutions already exist?\\n'},\n",
       " {'page_number': 8,\n",
       "  'text': 'Sources for solutions that already exist\\n•https://intuiville.intuiface.com/usage/voice -controlled -presentations -work\\n•https://www.presentations.ai/\\n'},\n",
       " {'page_number': 9,\n",
       "  'text': 'Why is our solution better?\\n•Jumping between slides with voice commands (Let ´s switch back \\nto the team slide)\\n•Automatically selects the righ slides during a presentation based \\non the presenters storyline\\n•Dynamically creates new slides based on request from the \\nspeaker inlcuding web search or company files as source'},\n",
       " {'page_number': 10,\n",
       "  'text': 'Team\\n•Meet the team\\n•Johannes, Sophia, Woojin, Ben'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_pages(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            text = \"Empty\"\n",
    "        pages.append({'page_number': i+1, 'text': text})\n",
    "    return pages\n",
    "\n",
    "pdf_pages = extract_pdf_pages(\"Makeathon TUM presentation.pdf\")\n",
    "\n",
    "pdf_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13e9b4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image is a vibrant, conceptual illustration representing artificial intelligence (AI), text extraction, and classification technology using **OpenCV** and **OpenAI**. Here are the key elements and details:\\n\\n### Central Visuals\\n- **Brain Illustration:** At the center, there\\'s an artistic drawing of a human brain made up of colorful, swirling segments that create a spectrum from purple, blue, and green on the left to yellow, orange, and red on the right.\\n  - The brain is designed with curving lines and dots, symbolizing neural networks and pathways, emphasizing the concept of artificial intelligence and data flow.\\n- **Icons and Graphics:** \\n  - An eye icon with a multicolored iris, surrounded by graphical elements, suggests computer vision and image recognition.\\n  - A triangular \"play\" button in OpenCV’s style indicates multimedia or processing capabilities.\\n  - A vertical bar chart with a rainbow gradient, symbolizing measuring or classifying data.\\n\\n### Text Elements\\n- The words \"AI\", \"OpenAI\", \"OpenCV\", \"OCV\" (short for OpenCV), and \"AI-driven Text Extraction and Classification using OpenCV\" are dispersed around the image. These texts emphasize the tools and the technological focus.\\n- The fonts are bold and modern, some in black, others multicolored to match the rainbow palette.\\n- The arrangement gives a dynamic, creative, and tech-savvy impression.\\n\\n### Color Scheme\\n- The overall color scheme is very bright and gradient-heavy, dominated by all colors of the rainbow, creating a lively and innovative feel.\\n- Background is light blue/purple, fading into white in places, giving a digital-ethereal effect.\\n\\n### Other Details\\n- Dots, circular shapes, lines, and spark-like bursts are scattered throughout, reinforcing the sense of interconnectivity, data points, and technological sophistication.\\n- The composition balances art with technical symbolism, bridging creativity with AI and computer vision.\\n\\n**Summary**: The image visually communicates the integration of AI, OpenAI, and OpenCV for tasks like text extraction and classification, represented through neural/metaphorical imagery, vivid colors, and bold technological motifs.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def describe_image_with_gpt4v(image_path):\n",
    "    \"\"\"Sends an image to GPT-4V and returns a description.\"\"\"\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as image:\n",
    "        # Convert the image to bytes\n",
    "        img_byte_array = io.BytesIO()\n",
    "        image.save(img_byte_array, format=\"PNG\")\n",
    "        img_bytes = img_byte_array.getvalue()\n",
    "\n",
    "    # Encode the image in base64\n",
    "    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "    # Prepare the image data in the required format\n",
    "    image_data = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send the image to GPT-4 Vision\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "                    image_data\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    # Extract and return the description\n",
    "    description = response.choices[0].message.content\n",
    "    return description\n",
    "\n",
    "\n",
    "describe_image_with_gpt4v(\"image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281fcec",
   "metadata": {},
   "source": [
    "### OpenAI Vision API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "275389e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Text: Empty...\n",
      "============================================================\n",
      "Page 2:\n",
      "Text: Flow\n",
      "Speaks2) Text Analyzer \n",
      "for keywords1) Speech to \n",
      "text converter3) Slide mover\n",
      "Presentation \n",
      "sl...\n",
      "============================================================\n",
      "Page 3:\n",
      "Text: Flow\n",
      "Speaks2) Text Analyzer \n",
      "for keywords1) Speech to \n",
      "text converter3a) Slide \n",
      "mover\n",
      "Presentation \n",
      "...\n",
      "============================================================\n",
      "Page 4:\n",
      "Text: Structure of presentation\n",
      "•Introduction (ideally something funny)\n",
      "•What problem is our product solvi...\n",
      "============================================================\n",
      "Page 5:\n",
      "Text: Introduction (ideally something funny)\n",
      "•30MM (Power Point) presentations are created per day\n",
      "•https:...\n",
      "============================================================\n",
      "Page 6:\n",
      "Text: What problem is our product solving?\n",
      "•Presentation have a fixed storyline and do not allow the prese...\n",
      "============================================================\n",
      "Page 7:\n",
      "Text: What solutions already exist?\n",
      "...\n",
      "Image 1 description: **Description:**\n",
      "The image presents Intuiface’s solution for \"Voice-Controlled Presentations,\" highl...\n",
      "Image 2 description: **Description:**  \n",
      "The image shows the logo for \"presentations.AI,\" a platform that uses artificial ...\n",
      "Image 3 description: **Description:**\n",
      "This image promotes an AI-powered presentation maker, branding itself as \"The World...\n",
      "============================================================\n",
      "Page 8:\n",
      "Text: Sources for solutions that already exist\n",
      "•https://intuiville.intuiface.com/usage/voice -controlled -...\n",
      "Image 1 description: The table compares four AI presentation tools based on features such as real-time slide creation, vo...\n",
      "============================================================\n",
      "Page 9:\n",
      "Text: Why is our solution better?\n",
      "•Jumping between slides with voice commands (Let ´s switch back \n",
      "to the ...\n",
      "============================================================\n",
      "Page 10:\n",
      "Text: Team\n",
      "•Meet the team\n",
      "•Johannes, Sophia, Woojin, Ben...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from each page of a PDF.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            text = \"Empty\"\n",
    "        pages.append({'page_number': i + 1, 'text': text})\n",
    "    return pages\n",
    "\n",
    "def extract_images_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts images from each page of a PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i in range(len(doc)):\n",
    "        for img in doc[i].get_images(full=True):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append({\"page_number\": i + 1, \"image\": image})\n",
    "    return images\n",
    "\n",
    "def describe_image_with_gpt4v(image):\n",
    "    \"\"\"Sends a PIL Image to GPT-4V and returns a description.\"\"\"\n",
    "    # Convert the image to bytes\n",
    "    img_byte_array = io.BytesIO()\n",
    "    image.save(img_byte_array, format=\"PNG\")\n",
    "    img_bytes = img_byte_array.getvalue()\n",
    "\n",
    "    # Encode the image in base64\n",
    "    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "    # Prepare the image data in the required format\n",
    "    image_data = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send the image to GPT-4 Vision\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \n",
    "                     # prompt for the image\n",
    "                     \"text\": \"Describe briefly, mention diagrams or tables and also include any relevant details. Keep it concise. \"\n",
    "                     \"This is used in combination with text from the PDF.\"}, \n",
    "                    image_data\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    # Extract and return the description\n",
    "    description = response.choices[0].message.content\n",
    "    return description\n",
    "\n",
    "def combine_text_and_images(pdf_path):\n",
    "    \"\"\"Combines text extraction and image descriptions page-by-page.\"\"\"\n",
    "    text_pages = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "\n",
    "    # Organize images by page number\n",
    "    images_by_page = {}\n",
    "    for img_data in images:\n",
    "        page_num = img_data[\"page_number\"]\n",
    "        if page_num not in images_by_page:\n",
    "            images_by_page[page_num] = []\n",
    "        images_by_page[page_num].append(img_data[\"image\"])\n",
    "\n",
    "    # Combine text + image descriptions\n",
    "    combined_pages = []\n",
    "    for page in text_pages:\n",
    "        page_number = page[\"page_number\"]\n",
    "        text = page[\"text\"]\n",
    "\n",
    "        image_descriptions = []\n",
    "        if page_number in images_by_page:\n",
    "            for image in images_by_page[page_number]:\n",
    "                description = describe_image_with_gpt4v(image)\n",
    "                image_descriptions.append(description)\n",
    "\n",
    "        combined_pages.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"text\": text,\n",
    "            \"image_descriptions\": image_descriptions\n",
    "        })\n",
    "\n",
    "    return combined_pages\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Makeathon TUM presentation.pdf\"\n",
    "final_pages = combine_text_and_images(pdf_path)\n",
    "\n",
    "# Print the combined result\n",
    "for page in final_pages:\n",
    "    print(f\"Page {page['page_number']}:\")\n",
    "    print(f\"Text: {page['text'][:100]}...\")  # Print first 100 chars\n",
    "    if page[\"image_descriptions\"]:\n",
    "        for idx, desc in enumerate(page[\"image_descriptions\"]):\n",
    "            print(f\"Image {idx+1} description: {desc[:100]}...\")  # Print first 100 chars\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92d545",
   "metadata": {},
   "source": [
    "### Huggingface: https://huggingface.co/sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b046ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')  # You can pick another\n",
    "# dim = model.get_sentence_embedding_dimension()  # <<<<<<<< GET DIMENSION EARLY\n",
    "# print(f\"Embedding dimension: {dim}\")\n",
    "\n",
    "# def embed_batch(pdf_pages):\n",
    "#     for page in pdf_pages:\n",
    "#         if page['text']:\n",
    "#             page['embedding'] = model.encode(page['text'])\n",
    "#     return [page['embedding'] for page in pdf_pages if 'embedding' in page]  # type: ignore\n",
    "\n",
    "# # After extracting pdf_pages\n",
    "# embeddings = embed_batch(pdf_pages)\n",
    "# print(f\"Embedded {len(embeddings)} pages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb1387",
   "metadata": {},
   "source": [
    "### OPENAI Alternative: https://platform.openai.com/docs/guides/embeddings?lang=python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "816265ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 10 pages, each a 1536-dim vector\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()                        # make sure OPENAI_API_KEY is in your .env\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()                   # reads api key from env\n",
    "\n",
    "def embed_batch(pdf_pages, batch_size: int = 16):\n",
    "    # 1) gather all page texts (empty string if missing)\n",
    "    texts = [page.get('text', '') for page in pdf_pages]\n",
    "\n",
    "    # 2) call the API in chunks of batch_size\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i : i + batch_size]\n",
    "        resp = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=chunk\n",
    "        )\n",
    "        # resp.data is a list of objects, each with an .embedding list\n",
    "        all_embeddings.extend([d.embedding for d in resp.data])\n",
    "\n",
    "    # 3) assign back to pages\n",
    "    for page, emb in zip(pdf_pages, all_embeddings):\n",
    "        if page.get('text'):\n",
    "            page['embedding'] = emb\n",
    "        else:\n",
    "            page['embedding'] = None\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# — example usage —\n",
    "# after you’ve built pdf_pages = [ {\"page_number\":1, \"text\": \"...\"} , ... ]\n",
    "embs = embed_batch(pdf_pages)\n",
    "dim = len(embs[0])  # <<<<<<<< GET DIMENSION EARLY\n",
    "print(f\"Embedded {len(embs)} pages, each a {len(embs[0])}-dim vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6cacfd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'delete_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client\u001b[38;5;241m.\u001b[39mdelete_collection(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'delete_collection'"
     ]
    }
   ],
   "source": [
    "client.delete_collection(collection_name=\"pdf_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691e484",
   "metadata": {},
   "source": [
    "### DELETE collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12043f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# client.delete_collection(collection_name=\"pdf_collection\")\n",
    "\n",
    "\n",
    "COLL = os.getenv(\"QDRANT_COLLECTION_NAME\", \"pdf_collection\")\n",
    "\n",
    "# 5) Create collection if it doesn’t exist\n",
    "collections = client.get_collections().collections\n",
    "exists = any(c.name == COLL for c in collections)\n",
    "\n",
    "if not exists:\n",
    "    client.create_collection(\n",
    "        collection_name=COLL,\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "237b7244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted 10 pages (dim=1536) into “pdf_collection2”.\n"
     ]
    }
   ],
   "source": [
    "# 6) Prepare your points and upsert\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=page[\"page_number\"],\n",
    "        vector=page[\"embedding\"],\n",
    "        payload={\n",
    "            \"text\":        page[\"text\"],\n",
    "            \"page_number\": page[\"page_number\"],\n",
    "        },\n",
    "    )\n",
    "    for page in pdf_pages\n",
    "]\n",
    "\n",
    "client.upsert(collection_name=COLL, points=points)\n",
    "print(f\"✅ Upserted {len(points)} pages (dim={dim}) into “{COLL}”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9715c67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 7   score=0.603\n",
      " → 'What solutions already exist? '\n",
      "\n",
      "Page 8   score=0.403\n",
      " → 'Sources for solutions that already exist •https://intuiville.intuiface.com/usage/voice -controlled -presentations -work •https://www.presentations.ai/ '\n",
      "\n",
      "Page 9   score=0.290\n",
      " → 'Why is our solution better? •Jumping between slides with voice commands (Let ´s switch back  to the team slide) •Automatically selects the righ slides during a presentation based  on the presenters st'\n",
      "\n",
      "Page 4   score=0.280\n",
      " → 'Structure of presentation •Introduction (ideally something funny) •What problem is our product solving? •What solutions already exist? •Why is our solution better? •How big is the business opportunity'\n",
      "\n",
      "Page 6   score=0.255\n",
      " → 'What problem is our product solving? •Presentation have a fixed storyline and do not allow the presenter  to adjust in -flight to needs of the audience. •Example 1: Professor realizes studes don ´t ha'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# — clients & globals —\n",
    "openai_client   = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "qdrant          = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\", \"pdf_collection\")\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 5):\n",
    "    # 1️⃣ Embed your query with OpenAI\n",
    "    resp = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=[query],\n",
    "    )\n",
    "    query_vector = resp.data[0].embedding\n",
    "\n",
    "    # 2️⃣ Fire the new Query API\n",
    "    qr = qdrant.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_vector,   # your dense vector\n",
    "        limit=top_k,          # how many neighbors to return\n",
    "        with_payload=True,    # pull back your stored page text & page_number\n",
    "    )                       \n",
    "\n",
    "    # 3️⃣ Extract the list of ScoredPoint objects\n",
    "    scored_points = qr.points  # type: ignore[attr-defined] :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "    # 4️⃣ Format the results\n",
    "    results = []\n",
    "    for sp in scored_points:\n",
    "        results.append({\n",
    "            \"score\":        sp.score,\n",
    "            \"page_number\":  sp.payload.get(\"page_number\"),\n",
    "            \"text_snippet\": sp.payload.get(\"text\", \"\")[:200].replace(\"\\n\", \" \")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    for r in semantic_search(\"What solution?\", top_k=5):\n",
    "        print(f\"Page {r['page_number']}   score={r['score']:.3f}\")\n",
    "        print(f\" → {r['text_snippet']!r}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
